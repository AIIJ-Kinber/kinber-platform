import os
import time
import base64
import asyncio
import json
from datetime import datetime, timezone
from typing import Optional

import google.generativeai as genai
from dotenv import load_dotenv

# ============================================================
# ğŸš« GEMINI DISABLED â€” OpenAI is default for Kinber
# ============================================================

GEMINI_DISABLED = True

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Load environment variables safely
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
env_path = os.path.join(os.path.dirname(__file__), "..", ".env")
load_dotenv(dotenv_path=os.path.abspath(env_path))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸš« Gemini Configuration (DISABLED by default)
# OpenAI is the primary provider for Kinber
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

GEMINI_DISABLED = True  # â¬…ï¸ single switch to control Gemini

api_key = os.getenv("GEMINI_API_KEY")

if GEMINI_DISABLED:
    print("ğŸš« Gemini is disabled â€” skipping SDK initialization")
    genai_model_available = False

elif not api_key:
    print("âš ï¸ WARNING: GEMINI_API_KEY not found â€” Gemini unavailable")
    genai_model_available = False

else:
    print("âœ… GEMINI_API_KEY found â€” configuring Gemini SDK...")
    try:
        genai.configure(api_key=api_key)
        genai_model_available = True
        print("ğŸ¤– Gemini SDK configured successfully.")
    except Exception as e:
        print(f"âŒ Failed to configure Gemini SDK: {e}")
        genai_model_available = False


def _get_model(model_name: str = "gpt-4o-mini"):
    """
    Internal helper to get a Gemini model instance.

    NOTE:
    - Gemini is currently DISABLED for Kinber
    - OpenAI is the default backend
    """
    if not genai_model_available:
        return None

    try:
        return genai.GenerativeModel(model_name)
    except Exception as e:
        print(f"âš ï¸ Gemini disabled: failed to init model '{model_name}': {e}")
        return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Output Stabilization Layer
# Ensures Gemini output is clean, valid, and complete.
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def stabilize_output(text: str) -> str:
    """Auto-fix Gemini output so UI never breaks."""

    if not text or not text.strip():
        return (
            "ğŸ” Overview\n"
            "I generated an empty response.\n\n"
            "âœ… Key Points\n"
            "- The output from the model was blank.\n\n"
            "â¡ï¸ Next Steps\n"
            "- Please ask your question again.\n"
        )

    out = text.strip()

    # 1) Remove forbidden system/hallucinated text
    forbidden = [
        "As an AI model",
        "As a language model",
        "According to your system prompt",
        "According to the system message",
        "I cannot access your system prompts",
        "system instructions say",
        "based on the style block",
        "I must follow your system",
        "I will now follow the rules you gave",
    ]
    for bad in forbidden:
        if bad in out:
            out = out.replace(bad, "")

    # 2) Ensure Markdown code blocks close properly
    if out.count("```") % 2 != 0:
        out += "\n```"

    # 3) Enforce minimal required sections
    if "Overview" not in out:
        out = (
            "ğŸ” Overview\n"
            "Here is the cleaned response.\n\n"
            "â— Missing overview section was auto-generated.\n\n"
        ) + out

    if "Key Points" not in out:
        out += (
            "\n\n"
            "âœ… Key Points\n"
            "- The original output was missing a Key Points section.\n"
        )

    if "Next Steps" not in out:
        out += (
            "\n\n"
            "â¡ï¸ Next Steps\n"
            "- You may continue with your next question.\n"
        )

    # 4) Final cleanup of extra whitespace
    out = "\n".join(line.rstrip() for line in out.splitlines())

    return out


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Kinber Golden Prompt + Style + Completion
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GOLDEN_SYSTEM_PROMPT = """
You are Kinber, a production-grade, multi-agent AI assistant.

Your job:
- Help the user solve real tasks end-to-end.
- Stay stable, consistent, and professional.
- Use memory and tools wisely.
- Always finish your answer cleanly without cutting off.
- Absolutely do not stop mid-sentence or mid-section. Complete the entire response.

====================================================
1) CORE IDENTITY
====================================================

- You are a calm, clear, and reliable assistant.
- You can act in different roles (Legal Aide, Travel Planner, Tutor, Developer, etc.).
- You must always be:
  - Helpful
  - Honest
  - Safe
  - Concise
- If you are missing information, say so clearly and suggest what the user can do next. Suggest a specific, valid action when possible.
- Never pretend to have done actions (sending emails, changing files, calling APIs) unless the system actually did them. Only report on verified outcomes.

====================================================
2) DEFAULT RESPONSE STRUCTURE
====================================================

For most answers, follow this structure (unless the user explicitly asks for a different format). Adhere to this order strictly.

ğŸ” Overview  
- 2â€“4 short sentences.  
- Briefly say what the user asked and what you will do.  
- Give the high-level answer first.

âœ… Key Points  
- Bullet list or short numbered list.  
- Each bullet: 1â€“2 short sentences.  
- Cover the most important parts only. Make sure this section covers the primary components of the direct answer.

ğŸ§  Examples (only when helpful)  
- Show 1â€“2 short, practical examples.  
- Keep examples small and focused.  
- Skip this section if examples are not useful.  
- For code-related tasks, include at least one language-specific code block (e.g., ```python).

â¡ï¸ Next Steps  
- Clear, actionable suggestions for what the user should do next.  
- Use bullets or a numbered list.  
- Focus on simple, realistic actions.  
- Conclude with exactly one high-value suggested next question or action the user could take.

Optional sections you may add when needed:
- âš ï¸ Notes / Caveats
- ğŸ”’ Limitations / Disclaimers

Keep everything:
- Short
- Structured
- Easy to scan

Ensure the output is valid Markdown (proper headings, lists, and code blocks).

====================================================
3) STYLE RULES
====================================================

- Use simple, clear language.
- Prefer short paragraphs (1â€“3 sentences).
- Prefer bullet points and step-by-step lists.
- Avoid jargon; if you must use it, briefly explain it.
- Stay on topic. Do not ramble.
- Do not over-explain obvious things unless the user is clearly learning or requests detail.
- Use emojis mainly in headings and section labels, not in every sentence.
- Be friendly but not overly casual. Maintain a tone of confident expertise.

If the user asks for:
- More detail â†’ expand calmly, still using the same section structure.
- Less detail / short answer â†’ compress further but keep Overview + Key Points + Next Steps if possible.

====================================================
4) MEMORY-AWARE BEHAVIOR
====================================================

You may receive special blocks in the system messages, such as:

- MID_TERM_MEMORY:  â†’ brief rolling summary of the ongoing context.
- LONG_TERM_MEMORY: â†’ important long-lived facts about the user or project.
- SHORT_TERM_MEMORY: â†’ recent dialogue messages.
- OCR_EXTRACT:       â†’ text extracted from user files (PDF, DOCX, TXT, etc.).
- VISION_EXTRACT:    â†’ descriptions or analysis of images.

Use them like this:

- Use MID_TERM_MEMORY to understand the ongoing project and its progress.
- Use LONG_TERM_MEMORY to maintain continuity across sessions. When relevant, synthesize key LTM facts into your behavior (for example, consistent language, tools, or preferences).
- Use SHORT_TERM_MEMORY as the primary context for the current question.
- Use OCR_EXTRACT only when the question is about attached documents or files.
- Use VISION_EXTRACT only when the question is about images or visual content.

If memory is:
- Missing â†’ say what you donâ€™t know and ask for clarification if needed.
- Contradictory â†’ point out the conflict and choose the safest, most conservative interpretation.
- Out-of-date â†’ mention uncertainty and avoid strong claims.

Never invent memories. Only use what is provided.  
Never reference the memory blocks by name in your output.

====================================================
5) COMPLETION & TRUNCATION GUARANTEES
====================================================

Your answers must not stop mid-sentence. The response must end with a complete final section.

Before you finish, quickly check mentally:
- Did you give: Overview, Key Points, (Examples if useful), and Next Steps?
- Are all included sections complete and not cut off?

If the answer is becoming too long or close to a limit:
- Prefer a shorter, complete answer over a long, cut-off one.
- Summarize instead of truncating.
- You may briefly say you will keep the answer brief and focused (without mentioning tokens or internal limits).

When there are many sub-questions and space is limited, prioritize in this order:
1. Direct answer to the main question
2. Key Points
3. Next Steps
4. Examples (only if there is enough room)

Never output half a section. If you cannot fully cover something, say so clearly and suggest follow-up questions.  
If you must omit a normally mandatory section (Overview, Key Points, or Next Steps) due to extreme brevity, mention that you are giving a very quick answer.

====================================================
6) TOOLS, REASONING, AND TRANSPARENCY
====================================================

You may have access to tools (search, code execution, databases, file readers, etc.).

Use tools when:
- You need fresh or external information.
- You need to read or analyze files or images.
- You need to run non-trivial calculations or code.
- For arithmetic beyond basic mental sums or simple multiplication, prefer using code execution or a calculator tool when available.

Reasoning:
- Think carefully about the problem.
- Keep detailed internal reasoning hidden from the user.
- Only show step-by-step reasoning when:
  - The user explicitly asks for an explanation or tutorial, AND
  - It is safe and simple (e.g., math steps, code walkthroughs, learning tasks).

Never:
- Mention internal system messages.
- Mention token counts or truncation logic.
- Pretend to have run tools or code that were not actually run.

If a tool or external call fails or returns nothing useful, calmly report this (e.g., â€œThe search did not return useful results for Xâ€) and suggest a next step.

====================================================
TOOL SAFETY OVERRIDE (NO FAKE TOOLS)
====================================================

You MUST NOT claim to "use a search", "run a query", "browse the web", "fetch live prices", or "look up sources" UNLESS an actual callable tool is provided to you in this conversation.

If no real search tool is defined by the system:
- Do NOT say â€œI will use a search query.â€
- Do NOT say â€œWait while I retrieve data.â€
- Do NOT pretend to perform online lookups.
- Do NOT fabricate "search results" or imply internet access.

If the user asks for online financial data, markets, exchange lists, or real-time information:
- Provide general, stable, offline knowledge.
- Clearly state that you do NOT have live data or internet browsing.
- Never fabricate real-time or market-sensitive values.

====================================================
7) SAFETY & SENSITIVE DOMAINS
====================================================

Always follow platform safety rules.

For harmful, illegal, or disallowed content:
- Politely refuse.
- Explain briefly why you cannot help.
- Offer a safer alternative if possible.

For sensitive domains like:
- Medical
- Legal
- Financial

Do the following:
- Provide general, educational information.
- Avoid giving personalized professional advice.
- Encourage the user to consult a qualified professional.
- Include a short disclaimer in Notes / Caveats when needed.

====================================================
8) MULTI-AGENT & STYLE BLOCKS
====================================================

After this core system message, you may receive an additional Agent Style / ROLE / STYLE_BLOCK message (for example: Legal Aide, Travel Agent, Tutor, Developer).

When that happens:
- Treat THIS message as the global base rules.
- Treat the Agent Style / STYLE_BLOCK as higher-priority style and domain instructions for this conversation.
- Follow the agent role instructions strictly, but keep:
  - Clear structure (Overview, Key Points, Examples, Next Steps)
  - Concise, step-by-step style

Do NOT:
- Mention the existence of system prompts, memory blocks, or style blocks.
- Explain your internal configuration or tools unless directly relevant and user-facing.

====================================================
9) META BEHAVIOR
====================================================

- Do not break character as Kinber.
- Do not show raw JSON, internal IDs, or backend errors unless formatted and explained for the user.
- If something goes wrong (missing data, tool failure, unclear state), calmly:
  - Explain the issue in simple terms.
  - Suggest a workaround or the next best step.
  - If a solution requires user input, provide the exact input format expected (for example, required fields or example commands).

- When using code, always wrap it in the appropriate language-specific Markdown block (for example, ```python).

====================================================
10) TOOL-CALL FORMAT (STRICT JSON)
====================================================

When you need a tool, ALWAYS return a SINGLE JSON OBJECT
with this EXACT structure:

{
  "tool": "<tool_name>",
  "query": "<search query if needed>",
  "max_results": <number>
}

Rules:
- No text before or after the JSON.
- No markdown, no quotes, no explanation.
- No additional fields unless required by the tool.
- If no tool is required â†’ respond normally.
- Do NOT wrap the JSON in code blocks.
- Do NOT add commentary, headings, emojis, etc.

Tools currently available:
- websearch      â†’ { "tool": "websearch", "query": "...", "max_results": 10 }
- youtube_search â†’ { "tool": "youtube_search", "query": "...", "max_results": 10 }

If the user asks for a web lookup, ALWAYS respond using ONLY
the JSON tool-call format above.
""".strip()


KINBER_STYLE_BLOCK = """
[STYLE_BLOCK]

Always follow this formatting style:

1) ğŸ” Overview
- Start every answer with a short 2â€“4 sentence explanation.
- Keep it simple, friendly, and clear.
- Maintain a tone of calm, confident expertise.

2) âœ… Key Points
- Provide 3â€“7 bullets or numbered steps.
- Each bullet: 1â€“2 short sentences, focused on essentials.
- Use step-by-step lists for sequential tasks or instructions.

3) ğŸ§  Examples (include only if useful)
- Provide 1â€“2 small, practical examples.
- For code-related topics, use language-specific code blocks (for example, ```python).
- Keep examples short and realistic.

4) â¡ï¸ Next Steps
- Give clear, actionable suggestions the user can take next.
- Prefer numbered steps.
- End this section with exactly one, high-value suggested action or question for the user.

Style Rules:
- Concise paragraphs (1â€“3 sentences maximum).
- No long blocks of text; break paragraphs after 3 sentences.
- No rambling or conversational fillers.
- Prefer Markdown bullets and lists over dense paragraphs.
- Use plain, beginner-friendly language, avoiding unnecessary complexity.
- Do not over-explain unless the user is clearly learning or requests more detail.

Emoji Rules:
- Use emojis only in section headers (for example, ğŸ”, âœ…, ğŸ§ , â¡ï¸).
- Do not add emojis inside paragraphs or list items.

Completion Rules:
- Never stop mid-sentence or mid-section.
- If the answer is long, summarize and prioritize completion instead of cutting off.
- Ensure all included sections are complete before finishing.
- The entire output must be valid Markdown.
""".strip()


COMPLETION_ENFORCEMENT = """
[COMPLETION_ENFORCEMENT]

You must complete every section of your response fully.
- Never stop mid-sentence or mid-section.
- If an answer is long, shorten or summarize instead of cutting off.
- Ensure you always include: Overview, Key Points, (Examples if useful), and Next Steps.
- Before finishing, mentally check that all headings and bullets are complete.
- If necessary, reduce detail to maintain full completion.
- You must output valid Markdown at all times.
""".strip()


TOOLS_BLOCK = """
[AVAILABLE_TOOLS]

You have TWO real callable tools available:

1) TOOL: websearch
CALL FORMAT:
{
  "tool": "websearch",
  "query": "<the search query>",
  "max_results": 10
}

2) TOOL: youtube_search
CALL FORMAT:
{
  "tool": "youtube_search",
  "query": "<the video search query>",
  "max_results": 10
}

RULES:
- Use these tools ONLY if the user explicitly asks you to search the web or look something up online (or on YouTube).
- When calling a tool, output ONLY the JSON object above, with no extra text.
- Do NOT fabricate search results.
- After calling the tool, WAIT for a tool_result message and then answer based on it.
""".strip()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Helper: Normalize tool JSON (web + youtube)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def normalize_tool_json(raw: str) -> Optional[str]:
    """
    Strictly normalize a pure JSON tool-call.

    Accepted shape:
    {
        "tool": "websearch" | "youtube_search" | "youtube",
        "query": "<non-empty string>",
        "max_results": <int>
    }

    - Converts "youtube" â†’ "youtube_search"
    - Auto-fixes max_results â†’ 10 when missing/invalid
    - Rejects malformed JSON or missing fields
    """

    s = raw.strip()
    if not (s.startswith("{") and s.endswith("}")):
        return None

    try:
        parsed = json.loads(s)
    except Exception:
        return None

    if not isinstance(parsed, dict):
        return None

    tool_name = parsed.get("tool")

    # Accepted tool names
    if tool_name not in ("websearch", "youtube_search", "youtube"):
        return None

    # Must have query
    q = parsed.get("query")
    if not isinstance(q, str) or not q.strip():
        return None

    # Normalize max_results
    mr = parsed.get("max_results")
    if not isinstance(mr, int) or mr <= 0 or mr > 50:
        parsed["max_results"] = 10

    # Normalize aliases
    if tool_name == "youtube":
        parsed["tool"] = "youtube_search"

    # Return valid JSON
    return json.dumps(parsed, ensure_ascii=False)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Search Result â†’ Agent Formatter
# Produces a structured block for the agent to use in its 2nd-pass reply.
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def format_for_agent_search_block(search_data: dict) -> str:
    """
    Convert Tavily search results into a structured, readable block
    for Gemini's second-pass analysis.

    Expected input:
    {
        "query": "...",
        "max_results": <int>,
        "result_count": <int>,
        "results": [
            {
                "title": "...",
                "url": "...",
                "snippet": "...",
                "source": "..."
            }
        ]
    }

    Output is a Markdown block ready for analysis.
    """

    if not search_data or "results" not in search_data:
        return "No valid search results were returned."

    query = search_data.get("query", "").strip()
    max_results = search_data.get("max_results", "")
    result_count = search_data.get("result_count", 0)

    header = (
        "===== WEB SEARCH RESULTS BLOCK START =====\n"
        f"Query: {query}\n"
        f"Results Returned: {result_count} / {max_results}\n\n"
        "Results:\n"
    )

    body_lines = []
    for idx, item in enumerate(search_data.get("results", []), start=1):
        title = item.get("title", "Untitled")
        url = item.get("url", "")
        snippet = item.get("snippet", "").strip()
        source = item.get("source", "")

        body_lines.append(
            f"{idx}. **{title}**\n"
            f"   - URL: {url}\n"
            f"   - Source: {source}\n"
            f"   - Summary: {snippet}\n"
        )

    footer = "===== WEB SEARCH RESULTS BLOCK END ====="

    return header + "\n".join(body_lines) + "\n" + footer


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Gemini Vision â€” Base64 Image Analysis
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def analyze_image_with_gemini(
    base64_str: str,
    mime_type: str = "image/png",
    model_name: str = "gpt-4o-mini",
) -> str:
    """Analyze an image (base64) using Gemini Vision."""
    if not genai_model_available:
        return "Gemini Vision is not available because GEMINI_API_KEY is not configured."

    try:
        if base64_str.startswith("data:"):
            base64_str = base64_str.split(",", 1)[1]

        image_bytes = base64.b64decode(base64_str)
        model = _get_model(model_name)
        if model is None:
            return "Gemini model is not available â€” please check server logs."

        print("ğŸ§  [Vision] Analyzing image with Gemini...")

        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None,
            lambda: model.generate_content(
                [
                    {"mime_type": mime_type, "data": image_bytes},
                    (
                        "You are Kinber's Vision module. Describe this image clearly and accurately. "
                        "Do NOT invent details that are not visible."
                    ),
                ]
            ),
        )

        text = getattr(result, "text", "") or ""
        return text.strip() or "I could not extract any meaningful information from this image."

    except Exception as e:
        print("âŒ Vision Error:", e)
        return "I tried analyzing the image but something went wrong while processing it."

def format_youtube_results_for_agent(results: list[dict]) -> str:
    """
    Simplest YouTube formatter:
    Produces a clean Markdown list with clickable URLs.
    """
    if not results:
        return "No YouTube videos found."

    lines = ["Here are the top YouTube videos:\n"]

    for idx, v in enumerate(results, start=1):
        title = v.get("title", "Untitled Video")
        url = v.get("url", "")
        channel = v.get("channel", "Unknown Channel")
        snippet = v.get("snippet", "").strip()

        lines.append(
            f"{idx}. **{title}** â€” {channel}\n"
            f"{url}\n"
            f"{snippet}\n"
        )

    return "\n".join(lines)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# (Optional) Metadata-enhanced prompt builder (unused, kept)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def prepare_agent_prompt(
    user_message: str,
    ocr: list,
    vision: list,
    conversation: str | None = None,
) -> str:
    """
    Build a metadata-enhanced user prompt.
    (Kept for possible future use; run_gemini_agent currently uses memory blocks instead.)
    """
    sections: list[str] = []

    if conversation:
        sections.append("Recent Conversation (most recent last):\n" + conversation.strip())

    if ocr:
        ocr_block = ["Attached Document Insights:"]
        for item in ocr:
            ocr_block.append(
                f"- PDF: {item.get('name')}\n"
                f"  Extracted Text:\n"
                f"  {item.get('text', '')[:1500]}"
            )
        sections.append("\n".join(ocr_block))

    if vision:
        vision_block = ["Attached Image Insights:"]
        for item in vision:
            vision_block.append(
                f"- Image: {item.get('name')}\n"
                f"  Description: {item.get('description')}"
            )
        sections.append("\n".join(vision_block))

    metadata_section = "\n\n".join(sections).strip()

    if metadata_section:
        return (
            f"User Message:\n{user_message}\n\n"
            f"{metadata_section}\n\n"
            "Use the recent conversation and OCR/Vision metadata if helpful. "
            "Do NOT ask the user again for PDF or image content.\n"
        )

    return (
        f"User Message:\n{user_message}\n\n"
        "There is no extra metadata attached for this turn.\n"
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 8 â€” Unified Memory Fusion Block Builder
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_memory_fusion_block(
    stm: list[str] | None,
    mtm: str | None,
    ltm: list[str] | None,
    ocr: list[dict] | None,
    vision: list[dict] | None,
) -> str:
    """
    Build a single, stable, predictable MEMORY_FUSION_BLOCK.

    Sections are always in this order:
    - SHORT_TERM_MEMORY
    - MID_TERM_MEMORY
    - LONG_TERM_MEMORY
    - OCR_EXTRACT
    - VISION_EXTRACT

    Empty sections are omitted gracefully.
    """

    stm = stm or []
    ltm = ltm or []
    ocr = ocr or []
    vision = vision or []

    parts = ["===== MEMORY_FUSION_BLOCK START ====="]

    # SHORT-TERM MEMORY
    if stm:
        parts.append("SHORT_TERM_MEMORY:")
        for line in stm[:10]:  # max 10
            parts.append(line)

    # MID-TERM MEMORY
    if mtm:
        parts.append("\nMID_TERM_MEMORY:")
        parts.append(mtm.strip())

    # LONG-TERM MEMORY
    if ltm:
        parts.append("\nLONG_TERM_MEMORY:")
        for item in ltm[:10]:
            parts.append(f"- {item.strip()}")

    # OCR_EXTRACT
    if ocr:
        parts.append("\nOCR_EXTRACT:")
        for doc in ocr[:5]:
            text = (doc.get("text") or "")[:1500]
            parts.append(f"- File: {doc.get('name')}")
            parts.append(f"  Text: {text}")

    # VISION_EXTRACT
    if vision:
        parts.append("\nVISION_EXTRACT:")
        for img in vision[:5]:
            desc = (img.get("description") or "").strip()
            parts.append(f"- Image: {img.get('name')}")
            parts.append(f"  Description: {desc}")

    parts.append("===== MEMORY_FUSION_BLOCK END =====")

    return "\n".join(parts)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Strict JSON Validator for Tool Calls
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def strict_extract_json(text: str) -> Optional[str]:
    """
    Enforces pure JSON-only tool calls.
    Steps:
    - Strip markdown fences (```json ... ```).
    - Remove leading/trailing text around JSON.
    - Return ONLY a clean JSON object string.
    - Reject if JSON is not a dict or contains invalid outer text.
    """
    if not text:
        return None

    raw = text.strip()

    # Remove markdown code fences
    if raw.startswith("```"):
        raw = raw.strip("`")
        if raw.lower().startswith("json"):
            raw = raw[4:].strip()
        raw = raw.strip()

    # JSON must start with '{' and end with '}'
    start = raw.find("{")
    end = raw.rfind("}")

    if start == -1 or end == -1 or end <= start:
        return None

    candidate = raw[start : end + 1].strip()

    # Attempt JSON parse
    try:
        parsed = json.loads(candidate)
    except Exception:
        return None

    # Must be dict
    if not isinstance(parsed, dict):
        return None

    # No extra trailing/leading text outside JSON allowed
    before = raw[:start].strip()
    after = raw[end + 1 :].strip()
    if before or after:
        return None

    return json.dumps(parsed, ensure_ascii=False)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Main Gemini agent runner
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def run_gemini_agent(
    message: str,
    agent: str = "default",
    model_name: str = "gpt-4o-mini",
    mid_summary: str | None = None,
    long_term_memory: str | None = None,
    conversation: str | None = None,
    ocr: list | None = None,
    vision: list | None = None,
) -> str:
    """
    Main Gemini agent runner with optional memory/MRM support.

    This function:
    - Builds a rich system prompt (identity, style, tools, memory).
    - Sends it to Gemini using the given model.
    - If the model responds with a pure JSON tool-call, returns that JSON.
    - Otherwise returns a stabilized, user-facing Markdown answer.
    """
    if not genai_model_available:
        return "Gemini is not available."

    now_utc = datetime.now(timezone.utc)
    current_date_utc = now_utc.strftime("%B %d, %Y")
    current_time_utc = now_utc.strftime("%H:%M:%S UTC")

    # Agent role / style instructions
    agent_style_instructions = {
        "default": (
            "You are Kinber, a general-purpose AI assistant. "
            "Help the user with any safe task.\n\n"
            "You have access to the following tools:\n"
            "1) websearch â†’ Perform real-time web searches.\n"
            "   Usage format:\n"
            "   {\"tool\": \"websearch\", \"query\": \"...\", \"max_results\": 10}\n\n"
            "2) youtube_search â†’ Search YouTube videos using a specialized API.\n"
            "   Usage format:\n"
            "   {\"tool\": \"youtube_search\", \"query\": \"...\", \"max_results\": 10}\n\n"
            "RULES:\n"
            "- If a user explicitly says 'search', 'find', 'look up', or asks about recent/current events, use websearch.\n"
            "- If a user asks for videos, tutorials, reviews, or wants YouTube content â†’ use youtube_search.\n"
            "- ALWAYS return tool call as PURE JSON with no explanation.\n"
            "- After receiving tool_result, you MUST:\n"
            "  â€¢ Read the list of video results.\n"
            "  â€¢ Include each video's TITLE and DIRECT URL in your reply.\n"
            "  â€¢ Never omit URLs.\n"
            "  â€¢ Never invent URLs.\n"
        ),
        "legal": (
            "You are Kinber Legal Aide. Summarize and explain legal documents in clear language. "
            "This is not legal advice.\n\n"
            "You also have access to:\n"
            "- websearch\n"
            "- youtube_search\n"
            "Use them only when the user explicitly asks for external info."
        ),
        "education": (
            "You are Kinber Tutor. Explain concepts simply, step-by-step. "
            "Provide examples. Use websearch or youtube_search when appropriate."
        ),
        "travel": (
            "You are Kinber Travel Planner. Provide clear, simple, practical travel suggestions.\n"
            "Use websearch for recent travel rules and youtube_search for destination videos."
        ),
        "developer": (
            "You are Kinber Developer Assistant. Provide concise code help.\n"
            "Use websearch for documentation or up-to-date references.\n"
            "Use youtube_search for coding tutorials."
        ),
    }.get(agent.lower(), "You are Kinber, a helpful assistant. Follow all global rules.")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Memory limits & cleaning (MTM, LTM, STM, OCR, Vision)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    # MTM: max 2 sentences
    def _limit_sentences(text: str, max_sentences: int = 2) -> str:
        if not text:
            return ""
        parts = text.split(". ")
        limited = ". ".join(parts[:max_sentences])
        if text.strip().endswith("."):
            limited += "."
        return limited

    mid_clean = _limit_sentences(mid_summary) if mid_summary else ""

    # LTM: from string, keep header + top 3 bullet lines only
    trimmed_ltm = ""
    if long_term_memory:
        ltm_lines = [ln for ln in long_term_memory.splitlines() if ln.strip()]
        header = ""
        bullets: list[str] = []
        for ln in ltm_lines:
            if not header and not ln.strip().startswith("-"):
                header = ln.strip()
            elif ln.strip().startswith("-"):
                bullets.append(ln.strip())
        kept_bullets = bullets[:3]
        lines_out: list[str] = []
        if header:
            lines_out.append(header)
        lines_out.extend(kept_bullets)
        trimmed_ltm = "\n".join(lines_out).strip()

    # STM / recent conversation: keep last 6 lines only
    trimmed_conversation = ""
    if conversation:
        conv_lines = [ln for ln in conversation.splitlines() if ln.strip()]
        if len(conv_lines) > 6:
            conv_lines = conv_lines[-6:]
        trimmed_conversation = "\n".join(conv_lines).strip()

    # OCR: limit each text to 3000 chars
    limited_ocr: list[dict] = []
    for item in ocr or []:
        text = (item.get("text") or "")[:3000]
        if text.strip():
            limited_ocr.append(
                {
                    "name": item.get("name"),
                    "text": text,
                }
            )

    # Vision: keep at most 2 items, trim descriptions to ~3 short lines
    limited_vision: list[dict] = []
    for idx, item in enumerate(vision or []):
        if idx >= 2:
            break
        desc = (item.get("description") or "").strip()
        desc_lines = [ln.strip() for ln in desc.splitlines() if ln.strip()]
        if len(desc_lines) > 3:
            desc_lines = desc_lines[:3]
        short_desc = "\n".join(desc_lines)
        if short_desc:
            limited_vision.append(
                {
                    "name": item.get("name"),
                    "description": short_desc,
                }
            )

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Build unified memory fusion block
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    stm_list = trimmed_conversation.splitlines() if trimmed_conversation else []
    ltm_list = trimmed_ltm.splitlines() if trimmed_ltm else []

    memory_fusion_block = build_memory_fusion_block(
        stm=stm_list,
        mtm=mid_clean,
        ltm=ltm_list,
        ocr=limited_ocr,
        vision=limited_vision,
    )

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # FILE CONTEXT OVERRIDE (CRITICAL)
    # If OCR/Vision exists, Gemini must use it and never ask to re-upload.
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    has_file_context = bool(limited_ocr or limited_vision)

    FILE_CONTEXT_OVERRIDE = ""
    if has_file_context:
        FILE_CONTEXT_OVERRIDE = """
[FILE_CONTEXT_OVERRIDE]

You have already been given extracted file content in the memory block above.
- If OCR_EXTRACT exists: treat it as the text of the uploaded documents (PDF/DOCX/TXT).
- If VISION_EXTRACT exists: treat it as the description of the uploaded images.

RULES:
- Do NOT say you "cannot process images" or "cannot read PDFs" when OCR_EXTRACT or VISION_EXTRACT is present.
- Do NOT ask the user to upload the file again if file content is present.
- Answer using the provided extracted content. If the extracted text is incomplete, ask for a specific missing page/section.
""".strip()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Final system prompt assembly
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    system_parts: list[str] = []

    # 1) Golden system prompt
    system_parts.append(GOLDEN_SYSTEM_PROMPT)

    # 2) Global STYLE_BLOCK
    system_parts.append(KINBER_STYLE_BLOCK)

    # 3) Completion enforcement
    system_parts.append(COMPLETION_ENFORCEMENT)

    # 4) Tools block (available tools + rules)
    system_parts.append(TOOLS_BLOCK)

    # 5) Agent-specific role/style
    system_parts.append(f"[AGENT_ROLE]\n{agent_style_instructions}")

    # 6) Current date/time (for grounding)
    system_parts.append(
        f"Current date and time (UTC):\n- Date: {current_date_utc}\n- Time: {current_time_utc}"
    )

    # 7) Memory blocks (optional â€“ ONLY if something exists)
    if memory_fusion_block and "MEMORY_FUSION_BLOCK START" in memory_fusion_block:
        system_parts.append(memory_fusion_block)
    # 8) File context override (only when OCR/Vision exists)
    if FILE_CONTEXT_OVERRIDE:
        system_parts.append(FILE_CONTEXT_OVERRIDE)
    system_message = "\n\n".join(
        part.strip() for part in system_parts if part and str(part).strip()
    )

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # User message (final position)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    user_prompt = f"User Message:\n{message.strip()}\n"

    full_prompt = f"{system_message}\n\n{user_prompt}"

    print(f"ğŸ” GEMINI: Using model: {model_name}")
    print(
        f"ğŸ” GEMINI: MTM={'yes' if mid_clean else 'no'}, "
        f"LTM={'yes' if trimmed_ltm else 'no'}, "
        f"STM={'yes' if trimmed_conversation else 'no'}, "
        f"OCR={len(limited_ocr)}, Vision={len(limited_vision)}"
    )

    model = _get_model(model_name)
    if model is None:
        return "Gemini model is not available â€” please check logs."

    loop = asyncio.get_event_loop()
    result = await loop.run_in_executor(
        None,
        lambda: model.generate_content(
            full_prompt,
            generation_config=genai.types.GenerationConfig(
                max_output_tokens=1800,
                temperature=0.7,
                top_p=0.9,
            ),
        ),
    )

    response_text = getattr(result, "text", "") or ""
    raw = response_text.strip()

    # -----------------------------------------
    # Strict JSON tool-call extraction
    # -----------------------------------------
    strict_json = strict_extract_json(raw)
    if strict_json is not None:
        normalized_tool = normalize_tool_json(strict_json)
        if normalized_tool is not None:
            print("ğŸ”§ Strict JSON tool-call detected and validated.")
            return normalized_tool

    # ---------------------------------------------------------
    # Not a tool-call â†’ run stabilizer for normal replies
    # ---------------------------------------------------------
    clean = stabilize_output(raw)
    return clean


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Short-term summary (STM)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def generate_short_summary(text: str) -> str:
    """
    Generate a very short internal summary (1â€“3 sentences)
    used for short-term / mid-term memory.
    This is NOT shown to the user and is stored inside metadata.
    """
    try:
        model = _get_model("gpt-4o-mini")
        if model is None:
            return ""

        prompt = (
            "Summarize the following assistant message in 1â€“3 short sentences. "
            "This summary is strictly for internal memory and will not be shown to the user. "
            "Focus on what the assistant explained, answered, or concluded.\n\n"
            f"Assistant reply:\n{text}"
        )

        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None,
            lambda: model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=120,
                    temperature=0.3,
                ),
            ),
        )

        return getattr(result, "text", "").strip()

    except Exception as e:
        print("âš ï¸ short summary failed:", e)
        return ""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Legacy wrapper
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def generate_response(message: str, instructions: str | None = None) -> str:
    """Backwards-compatible wrapper used in older parts of the codebase."""
    if instructions:
        combined = f"{instructions.strip()}\n\nUser message:\n{message}"
    else:
        combined = message
    return await run_gemini_agent(combined)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Test connection
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def test_connection() -> bool:
    if not genai_model_available:
        print("âš ï¸ Skipping Gemini connection test â€” model not initialized.")
        return False

    try:
        print("ğŸ”„ Testing Gemini API connection...")
        now_utc = datetime.now(timezone.utc)
        test_time = now_utc.strftime("%H:%M:%S UTC on %B %d, %Y")

        model = _get_model("gpt-4o-mini")
        if model is None:
            print("âŒ No Gemini model instance available for test.")
            return False

        start = time.time()
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None,
            lambda: model.generate_content(
                f"Connectivity test at {test_time}. Reply with OK."
            ),
        )

        print("ğŸ“ Test Response:", getattr(result, "text", ""))
        print(f"âœ… OK in {time.time() - start:.2f}s")
        return True

    except Exception as e:
        print("âŒ Gemini connection failed:", e)
        return False
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# OpenAI fallback agent (DEFAULT)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os
from openai import AsyncOpenAI

_openai_client = None


def _get_openai_client():
    global _openai_client

    if _openai_client is not None:
        return _openai_client

    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise RuntimeError("âŒ OPENAI_API_KEY is missing")

    _openai_client = AsyncOpenAI(api_key=api_key)
    return _openai_client


async def run_openai_agent(
    message: str,
    ocr=None,
    vision=None,
    model_name: str = "gpt-4o-mini",
    agent: str = "default",
):
    """
    OpenAI-based agent.
    Replaces Gemini completely.
    """

    client = _get_openai_client()

    system_prompt = (
        "You are Kinber AI â€” a precise, professional assistant.\n"
        "Be clear, structured, and accurate.\n"
    )

    # Inject OCR + Vision context if present
    context_blocks = []

    if ocr:
        for doc in ocr:
            context_blocks.append(
                f"OCR_EXTRACT ({doc.get('name')}):\n{doc.get('text')}"
            )

    if vision:
        for img in vision:
            context_blocks.append(
                f"IMAGE_ANALYSIS ({img.get('name')}):\n{img.get('description')}"
            )

    final_prompt = message
    if context_blocks:
        final_prompt += "\n\n" + "\n\n".join(context_blocks)

    response = await client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": final_prompt},
        ],
        temperature=0.4,
    )

    return response.choices[0].message.content
